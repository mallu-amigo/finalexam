{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c4fa9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example texts and labels\n",
    "examples = [\n",
    "    \"I love this product! It's fantastic.\",\n",
    "    \"Terrible experience — won't buy again.\"\n",
    "]\n",
    "labels = [1, 0]  # 1: positive, 0: negative\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text, stop_words):\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Clean the examples\n",
    "cleaned_examples = [clean_text(text, ENGLISH_STOP_WORDS) for text in examples]\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(cleaned_examples)\n",
    "\n",
    "# Train Logistic Regression model (set solver and max_iter to be safe)\n",
    "model = LogisticRegression(solver='liblinear', random_state=0, max_iter=1000)\n",
    "model.fit(X, labels)\n",
    "\n",
    "# Example prediction\n",
    "new_texts = [\"I really like this!\", \"I hate this product\"]\n",
    "cleaned_new = [clean_text(t, ENGLISH_STOP_WORDS) for t in new_texts]\n",
    "X_new = vectorizer.transform(cleaned_new)\n",
    "print(model.predict(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc45f3",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is often preferred over a simple Bag of Words (Count Vectorizer) for text classification because it not only considers the frequency of words in a document but also accounts for how common or rare those words are across all documents. While Bag of Words simply counts word occurrences, TF-IDF reduces the weight of frequently occurring words that may not carry significant meaning (such as \"the\", \"is\", \"and\") and increases the importance of rarer, more informative words. This helps models focus on the most relevant terms for distinguishing between classes, leading to improved classification performance and reduced impact from common, less informative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ac476ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAB-USER-01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\LAB-USER-01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\LAB-USER-01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Example: replace with your data\n",
    "reviews = [\n",
    "    \"I love this product! Works great and arrived fast.\",\n",
    "    \"Terrible quality, broke after a week. Do not recommend.\",\n",
    "    \"Amazing value for money, highly recommended.\",\n",
    "    \"Not what I expected. Very disappointed.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]\n",
    "# Tokenize\n",
    "tokenized_reviews = [simple_preprocess(r, deacc=True) for r in reviews]\n",
    "# Train Word2Vec\n",
    "vector_size = 100\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_reviews,\n",
    "    vector_size=vector_size,\n",
    "    window=5,\n",
    "    min_count=1,    # adjust for your corpus size\n",
    "    workers=4,\n",
    "    seed=42,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "def avg_w2v_vector(tokens, model, vector_size):\n",
    "    vecs = [model.wv[t] for t in tokens if t in model.wv.key_to_index]\n",
    "    if len(vecs) > 0:\n",
    "        return np.mean(vecs, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size, dtype=float)\n",
    "\n",
    "X_w2v = np.vstack([avg_w2v_vector(toks, w2v_model, vector_size) for toks in tokenized_reviews])\n",
    "\n",
    "# Train/test split so evaluation is meaningful\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w2v, labels, test_size=2, random_state=42, stratify=labels)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99857192",
   "metadata": {},
   "source": [
    "One key advantage of using Word2Vec embeddings over TF-IDF is that Word2Vec captures semantic relationships between words by representing them in a continuous vector space, allowing similar words to have similar vector representations. This enables models to understand context and word meaning beyond simple frequency counts, whereas TF-IDF only reflects how often words appear and does not capture semantic similarity or context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44113229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAB-USER-01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 895ms/step - accuracy: 0.5000 - loss: 0.6935\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6930\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6925\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.6919\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6914\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.6907\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.6900\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5000 - loss: 0.6892\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.6882\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.6870\n",
      "Test accuracy: 0.5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    1.0000    0.6667         1\n",
      "           1     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.5000         2\n",
      "   macro avg     0.2500    0.5000    0.3333         2\n",
      "weighted avg     0.2500    0.5000    0.3333         2\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Text: \"This is the best purchase I've made.\" => Predicted: negative\n",
      "Text: 'Completely useless and poor quality.' => Predicted: negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAB-USER-01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\LAB-USER-01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\LAB-USER-01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Full example: LSTM-based sentiment classifier\n",
    "# Replace the sample `reviews` and `labels` with your own dataset as needed.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Reproducibility (note: full determinism depends on hardware / TF config)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# -----------------------\n",
    "# Example data (replace)\n",
    "# -----------------------\n",
    "reviews = [\n",
    "    \"I love this product! Works great and arrived fast.\",\n",
    "    \"Terrible quality, broke after a week. Do not recommend.\",\n",
    "    \"Amazing value for money, highly recommended.\",\n",
    "    \"Not what I expected. Very disappointed.\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# Optional new texts to predict after training\n",
    "new_texts = [\n",
    "    \"This is the best purchase I've made.\",\n",
    "    \"Completely useless and poor quality.\"\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# Parameters\n",
    "# -----------------------\n",
    "max_words = 1000       # maximum vocabulary size to keep (top words)\n",
    "max_len = 20           # max tokens per review (longer gets truncated)\n",
    "embedding_dim = 64     # embedding vector size\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "test_prop = 0.25\n",
    "random_state = 42\n",
    "\n",
    "# -----------------------\n",
    "# Tokenize & pad\n",
    "# -----------------------\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "X_seq = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post').astype('int32')\n",
    "y = np.array(labels, dtype='int32')\n",
    "\n",
    "# Safe vocab size for Embedding layer\n",
    "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "\n",
    "# -----------------------\n",
    "# Adaptive train/test split (handles very small datasets)\n",
    "# -----------------------\n",
    "n_samples = len(y)\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "if n_samples * test_prop < n_classes:\n",
    "    # Choose integer test size at least equal to n_classes, but leave at least 1 sample for training\n",
    "    test_size_int = min(max(n_classes, 1), max(1, n_samples - 1))\n",
    "    if test_size_int >= n_samples:\n",
    "        # As a last resort, fall back to a non-stratified proportional split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_seq, y, test_size=test_prop, random_state=random_state, stratify=None\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_seq, y, test_size=test_size_int, random_state=random_state, stratify=y\n",
    "        )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_seq, y, test_size=test_prop, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "# -----------------------\n",
    "# Build the LSTM model\n",
    "# -----------------------\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# -----------------------\n",
    "# Training (use validation split only if training set is large enough)\n",
    "# -----------------------\n",
    "use_validation = (len(X_train) >= 10)\n",
    "callbacks = []\n",
    "if use_validation:\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)]\n",
    "    val_split = 0.1\n",
    "else:\n",
    "    val_split = 0.0  # no validation split for very small training sets\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=val_split,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size if len(X_train) >= batch_size else max(1, len(X_train)),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Evaluate on test set\n",
    "# -----------------------\n",
    "if len(X_test) > 0:\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    print(\"Classification report on test set:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "else:\n",
    "    print(\"No test set available (too small dataset). Skipping evaluation.\")\n",
    "\n",
    "# -----------------------\n",
    "# Predict on new_texts (if provided)\n",
    "# -----------------------\n",
    "if 'new_texts' in globals() and new_texts:\n",
    "    new_seq = tokenizer.texts_to_sequences(new_texts)\n",
    "    X_new_seq = pad_sequences(new_seq, maxlen=max_len, padding='post', truncating='post').astype('int32')\n",
    "    preds_prob = model.predict(X_new_seq)\n",
    "    preds = (preds_prob > 0.5).astype(int).flatten()\n",
    "    for text, p in zip(new_texts, preds):\n",
    "        label = 'positive' if p == 1 else 'negative'\n",
    "        print(f\"Text: {text!r} => Predicted: {label}\")\n",
    "else:\n",
    "    print(\"No new_texts variable found or it is empty; skipping prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d720195d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_12 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │         \u001b[38;5;34m1,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,109</span> (168.40 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,109\u001b[0m (168.40 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,369</span> (56.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,369\u001b[0m (56.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,740</span> (112.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m28,740\u001b[0m (112.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9721ae2",
   "metadata": {},
   "source": [
    "LSTMs are often preferred over simple RNNs for text classification because they handle long-range dependencies and the vanishing-gradient problem much better. During backpropagation through time, standard RNNs tend to suffer from vanishing (or exploding) gradients, which prevents them from learning relationships across many time steps. LSTM units introduce a cell state plus gating mechanisms (input, forget, and output gates) that regulate information flow and create a more stable path for gradients (the “constant error carousel”). As a result, LSTMs can preserve and learn useful contextual information over longer sequences, making them more reliable for tasks where distant words influence the label. The trade-off is that LSTMs have more parameters and cost more compute, but their improved ability to capture long-term context usually yields better accuracy on real-world text problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27a58618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\lab-user-01\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\LAB-USER-01\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    " %pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c38b8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 4; class distribution: Counter({1: 2, 0: 2})\n",
      "Train samples: 2; Test samples: 2; test class distribution: Counter({1: 1, 0: 1})\n",
      "\n",
      "TF-IDF + LogisticRegression evaluation:\n",
      "Accuracy: 0.5 F1: 0.6666666666666666 ROC-AUC: 0.5\n",
      "\n",
      "Word2Vec(avg) + RandomForest evaluation:\n",
      "Accuracy: 1.0 F1: 1.0 ROC-AUC: 1.0\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAB-USER-01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858ms/step - accuracy: 0.5000 - loss: 0.6931\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5000 - loss: 0.6932\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5000 - loss: 0.6931\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5000 - loss: 0.6932\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5000 - loss: 0.6932\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5000 - loss: 0.6931\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.6931\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5000 - loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5000 - loss: 0.6931\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.6931\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\n",
      "LSTM (Embedding + LSTM) evaluation:\n",
      "Accuracy: 0.5 F1: 0.6666666666666666 ROC-AUC: 0.0\n",
      "\n",
      "Model comparison table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF + LogisticRegression</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word2Vec(avg) + RandomForest</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM (Embedding + LSTM)</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  accuracy      f1  roc_auc\n",
       "0   TF-IDF + LogisticRegression       0.5  0.6667      0.5\n",
       "1  Word2Vec(avg) + RandomForest       1.0  1.0000      1.0\n",
       "2       LSTM (Embedding + LSTM)       0.5  0.6667      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate three text-classification workflows on a common test set:\n",
    "# 1) TF-IDF + Logistic Regression\n",
    "# 2) Word2Vec (averaged) + Random Forest\n",
    "# 3) LSTM (Embedding + LSTM)\n",
    "#\n",
    "# The script:\n",
    "# - uses `reviews` and `labels` from the notebook environment if present,\n",
    "#   otherwise falls back to a small example dataset.\n",
    "# - creates a single train/test split (adaptive to tiny datasets)\n",
    "# - trains each model on the TRAIN set only\n",
    "# - evaluates on the TEST set using Accuracy, F1-score, and ROC-AUC\n",
    "# - builds a pandas DataFrame comparing the three models\n",
    "#\n",
    "# NOTE: If TensorFlow or gensim are not installed in the current kernel, the\n",
    "# corresponding model training will be skipped and results set to NaN.\n",
    "# Paste this cell into your notebook and run it (it is self-contained).\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional libs that may not be installed in every environment\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    from gensim.utils import simple_preprocess\n",
    "    gensim_available = True\n",
    "except Exception:\n",
    "    gensim_available = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    tf_available = True\n",
    "except Exception:\n",
    "    tf_available = False\n",
    "\n",
    "# ---------------------------\n",
    "# Load reviews & labels\n",
    "# ---------------------------\n",
    "if 'reviews' in globals() and 'labels' in globals():\n",
    "    X_all = reviews\n",
    "    y_all = np.array(labels)\n",
    "else:\n",
    "    # Fallback toy dataset (replace this by your dataset in the notebook)\n",
    "    X_all = [\n",
    "        \"I love this product! Works great and arrived fast.\",\n",
    "        \"Terrible quality, broke after a week. Do not recommend.\",\n",
    "        \"Amazing value for money, highly recommended.\",\n",
    "        \"Not what I expected. Very disappointed.\",\n",
    "        \"Exceeded my expectations, very happy with the purchase.\",\n",
    "        \"Poor build. Stopped working after two days.\"\n",
    "    ]\n",
    "    y_all = np.array([1, 0, 1, 0, 1, 0])\n",
    "\n",
    "# Basic checks\n",
    "if len(X_all) != len(y_all):\n",
    "    raise ValueError(\"reviews and labels must have the same length\")\n",
    "\n",
    "print(f\"Total samples: {len(X_all)}; class distribution: {Counter(y_all)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Adaptive train/test split\n",
    "# ---------------------------\n",
    "def adaptive_train_test_split(X, y, test_prop=0.25, random_state=42):\n",
    "    n_samples = len(y)\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    # if proportion would produce fewer test samples than classes, choose integer test size\n",
    "    if n_samples * test_prop < n_classes:\n",
    "        test_size_int = min(max(n_classes, 1), max(1, n_samples - 1))\n",
    "        if test_size_int >= n_samples:\n",
    "            # fallback to non-stratified fractional split\n",
    "            return train_test_split(X, y, test_size=test_prop, random_state=random_state, stratify=None)\n",
    "        else:\n",
    "            return train_test_split(X, y, test_size=test_size_int, random_state=random_state, stratify=y)\n",
    "    else:\n",
    "        return train_test_split(X, y, test_size=test_prop, random_state=random_state, stratify=y)\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = adaptive_train_test_split(X_all, y_all, test_prop=0.25, random_state=42)\n",
    "print(f\"Train samples: {len(X_train_text)}; Test samples: {len(X_test_text)}; test class distribution: {Counter(y_test)}\")\n",
    "\n",
    "# Prepare a results dict\n",
    "results = {\n",
    "    'model': [],\n",
    "    'accuracy': [],\n",
    "    'f1': [],\n",
    "    'roc_auc': []\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# 1) TF-IDF + Logistic Regression\n",
    "# ---------------------------\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test_tfidf)\n",
    "y_proba_lr = lr.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "f1_lr = f1_score(y_test, y_pred_lr, zero_division=0)\n",
    "# ROC-AUC requires both classes present in y_test\n",
    "try:\n",
    "    roc_lr = roc_auc_score(y_test, y_proba_lr)\n",
    "except Exception:\n",
    "    roc_lr = float('nan')\n",
    "\n",
    "results['model'].append('TF-IDF + LogisticRegression')\n",
    "results['accuracy'].append(acc_lr)\n",
    "results['f1'].append(f1_lr)\n",
    "results['roc_auc'].append(roc_lr)\n",
    "\n",
    "print(\"\\nTF-IDF + LogisticRegression evaluation:\")\n",
    "print(\"Accuracy:\", acc_lr, \"F1:\", f1_lr, \"ROC-AUC:\", roc_lr)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Word2Vec (averaged) + RandomForest\n",
    "# ---------------------------\n",
    "if not gensim_available:\n",
    "    warnings.warn(\"gensim not available: skipping Word2Vec model and filling NaNs\")\n",
    "    results['model'].append('Word2Vec(avg) + RandomForest')\n",
    "    results['accuracy'].append(float('nan'))\n",
    "    results['f1'].append(float('nan'))\n",
    "    results['roc_auc'].append(float('nan'))\n",
    "else:\n",
    "    # Tokenize using gensim.simple_preprocess (lowercase, deacc)\n",
    "    tokenized_train = [simple_preprocess(t, deacc=True) for t in X_train_text]\n",
    "    tokenized_test = [simple_preprocess(t, deacc=True) for t in X_test_text]\n",
    "\n",
    "    # Train Word2Vec on training tokens only\n",
    "    w2v_size = 100\n",
    "    w2v = Word2Vec(sentences=tokenized_train, vector_size=w2v_size, window=5, min_count=1, workers=4, seed=42, epochs=10)\n",
    "\n",
    "    # Function to average vectors\n",
    "    def avg_w2v_vec(tokens, model, vector_size):\n",
    "        vecs = [model.wv[t] for t in tokens if t in model.wv.key_to_index]\n",
    "        if len(vecs) > 0:\n",
    "            return np.mean(vecs, axis=0)\n",
    "        else:\n",
    "            return np.zeros(vector_size, dtype=float)\n",
    "\n",
    "    X_train_w2v = np.vstack([avg_w2v_vec(toks, w2v, w2v_size) for toks in tokenized_train])\n",
    "    X_test_w2v = np.vstack([avg_w2v_vec(toks, w2v, w2v_size) for toks in tokenized_test])\n",
    "\n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train_w2v, y_train)\n",
    "\n",
    "    y_pred_rf = rf.predict(X_test_w2v)\n",
    "    y_proba_rf = rf.predict_proba(X_test_w2v)[:, 1]\n",
    "\n",
    "    acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    f1_rf = f1_score(y_test, y_pred_rf, zero_division=0)\n",
    "    try:\n",
    "        roc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "    except Exception:\n",
    "        roc_rf = float('nan')\n",
    "\n",
    "    results['model'].append('Word2Vec(avg) + RandomForest')\n",
    "    results['accuracy'].append(acc_rf)\n",
    "    results['f1'].append(f1_rf)\n",
    "    results['roc_auc'].append(roc_rf)\n",
    "\n",
    "    print(\"\\nWord2Vec(avg) + RandomForest evaluation:\")\n",
    "    print(\"Accuracy:\", acc_rf, \"F1:\", f1_rf, \"ROC-AUC:\", roc_rf)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) LSTM (Embedding + LSTM)\n",
    "# ---------------------------\n",
    "if not tf_available:\n",
    "    warnings.warn(\"TensorFlow / Keras not available: skipping LSTM model and filling NaNs\")\n",
    "    results['model'].append('LSTM (Embedding + LSTM)')\n",
    "    results['accuracy'].append(float('nan'))\n",
    "    results['f1'].append(float('nan'))\n",
    "    results['roc_auc'].append(float('nan'))\n",
    "else:\n",
    "    # Reproducibility (best-effort)\n",
    "    np.random.seed(42)\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    # Tokenizer fit on training text only\n",
    "    max_words = 5000\n",
    "    max_len = 50\n",
    "    embedding_dim = 100\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "    # sequences\n",
    "    seq_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "    seq_test = tokenizer.texts_to_sequences(X_test_text)\n",
    "    X_train_seq = pad_sequences(seq_train, maxlen=max_len, padding='post', truncating='post').astype('int32')\n",
    "    X_test_seq = pad_sequences(seq_test, maxlen=max_len, padding='post', truncating='post').astype('int32')\n",
    "\n",
    "    # compute vocab size safely\n",
    "    vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "\n",
    "    # Build model\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "        LSTM(64),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training parameters: keep small to avoid long runs in notebooks\n",
    "    batch_size = 32 if len(X_train_seq) >= 32 else max(1, len(X_train_seq))\n",
    "    epochs = 10\n",
    "    callbacks = []\n",
    "    val_split = 0.1 if len(X_train_seq) >= 10 else 0.0\n",
    "    if val_split > 0:\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)]\n",
    "\n",
    "    # Fit model (suppress verbose to keep output readable)\n",
    "    model.fit(X_train_seq, y_train, validation_split=val_split, epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=1)\n",
    "\n",
    "    # Predictions and probabilities\n",
    "    y_proba_lstm = model.predict(X_test_seq).reshape(-1)\n",
    "    y_pred_lstm = (y_proba_lstm > 0.5).astype(int)\n",
    "\n",
    "    acc_lstm = accuracy_score(y_test, y_pred_lstm)\n",
    "    f1_lstm = f1_score(y_test, y_pred_lstm, zero_division=0)\n",
    "    try:\n",
    "        roc_lstm = roc_auc_score(y_test, y_proba_lstm)\n",
    "    except Exception:\n",
    "        roc_lstm = float('nan')\n",
    "\n",
    "    results['model'].append('LSTM (Embedding + LSTM)')\n",
    "    results['accuracy'].append(acc_lstm)\n",
    "    results['f1'].append(f1_lstm)\n",
    "    results['roc_auc'].append(roc_lstm)\n",
    "\n",
    "    print(\"\\nLSTM (Embedding + LSTM) evaluation:\")\n",
    "    print(\"Accuracy:\", acc_lstm, \"F1:\", f1_lstm, \"ROC-AUC:\", roc_lstm)\n",
    "\n",
    "# ---------------------------\n",
    "# Summary comparison table\n",
    "# ---------------------------\n",
    "df_results = pd.DataFrame(results)\n",
    "# Format numeric columns to 4 decimal places where possible\n",
    "for col in ['accuracy', 'f1', 'roc_auc']:\n",
    "    df_results[col] = df_results[col].apply(lambda v: (float(v) if (v is not None and not (isinstance(v, float) and np.isnan(v))) else np.nan))\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"\\nModel comparison table:\")\n",
    "display(df_results)    # in notebook, this renders nicely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9536640",
   "metadata": {},
   "source": [
    "I recommend deploying the Word2Vec (averaged) + Random Forest model because it achieved the best metrics and offers fast inference (cheap averaging + efficient tree inference) and moderate training cost compared with an LSTM, but only after you validate the result with stratified k‑fold CV and checks for overfitting or data leakage; if the RF’s perfect scores don’t hold up under robust validation, use TF‑IDF + Logistic Regression instead as a safer, more interpretable and cheaper-to-deploy baseline (and in either case calibrate probabilities and add monitoring)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "880b78d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Word2Vec -> ./w2v.pkl\n",
      "Saved RandomForest -> ./rf_w2v.pkl\n",
      "Verification prediction: Positive (prob=0.610)\n",
      "Models saved successfully in project root. You can now run the Streamlit app which will load './w2v.pkl' and './rf_w2v.pkl'.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Save trained gensim Word2Vec and scikit-learn RandomForest objects as pickle files\n",
    "in the project root. This script mirrors the behavior of the original joblib/gensim\n",
    "save flow but uses Python's pickle format.\n",
    "\n",
    "Paste & run this cell in your notebook (after training the models) or run as a script\n",
    "in the same environment where the trained objects exist in memory.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# Filenames saved in project root (no \"models/\" subfolder)\n",
    "W2V_FILENAME = \"w2v.pkl\"\n",
    "RF_FILENAME = \"rf_w2v.pkl\"\n",
    "\n",
    "# Candidate names for in-memory objects (adjust if your variables use different names)\n",
    "w2v_candidates = ['w2v', 'w2v_model', 'word2vec', 'w2vmodel', 'w2v_trained']\n",
    "rf_candidates = ['rf', 'rf_clf', 'rf_model', 'random_forest', 'rf_w2v']\n",
    "\n",
    "w2v_obj = None\n",
    "rf_obj = None\n",
    "\n",
    "for n in w2v_candidates:\n",
    "    if n in globals():\n",
    "        w2v_obj = globals()[n]\n",
    "        break\n",
    "\n",
    "for n in rf_candidates:\n",
    "    if n in globals():\n",
    "        rf_obj = globals()[n]\n",
    "        break\n",
    "\n",
    "if w2v_obj is None or rf_obj is None:\n",
    "    raise RuntimeError(\n",
    "        \"Could not find Word2Vec and/or RandomForest objects in this notebook under common names.\\n\"\n",
    "        \"Expected Word2Vec in one of: {}\\nExpected RF in one of: {}\\n\"\n",
    "        \"If your variables use different names, either rename them or set variables `w2v` and `rf` and re-run.\".format(w2v_candidates, rf_candidates)\n",
    ")\n",
    "\n",
    "# Save Word2Vec (pickle) and RF (pickle) to project root\n",
    "try:\n",
    "    # Ensure we write to the current working directory (project root)\n",
    "    w2v_path = os.path.join(os.getcwd(), W2V_FILENAME)\n",
    "    rf_path = os.path.join(os.getcwd(), RF_FILENAME)\n",
    "\n",
    "    # Save gensim Word2Vec object as pickle\n",
    "    with open(w2v_path, \"wb\") as f:\n",
    "        pickle.dump(w2v_obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Save RF along with vector_size metadata (if available)\n",
    "    vector_size = getattr(w2v_obj, \"vector_size\", None)\n",
    "    if vector_size is None:\n",
    "        # gensim <=4 stores size at wv.vector_size\n",
    "        vector_size = getattr(getattr(w2v_obj, \"wv\", None), \"vector_size\", None)\n",
    "\n",
    "    rf_data = {\"clf\": rf_obj, \"vector_size\": vector_size}\n",
    "    with open(rf_path, \"wb\") as f:\n",
    "        pickle.dump(rf_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(f\"Saved Word2Vec -> ./{W2V_FILENAME}\")\n",
    "    print(f\"Saved RandomForest -> ./{RF_FILENAME}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to save models: \" + str(e))\n",
    "\n",
    "# Quick verification: load back and run a sample prediction (requires gensim for preprocessing)\n",
    "try:\n",
    "    from gensim.utils import simple_preprocess\n",
    "except Exception:\n",
    "    raise RuntimeError(\"gensim required for verification. Install gensim in this kernel and re-run.\")\n",
    "\n",
    "try:\n",
    "    # Load pickle files\n",
    "    with open(w2v_path, \"rb\") as f:\n",
    "        w2v_loaded = pickle.load(f)\n",
    "\n",
    "    with open(rf_path, \"rb\") as f:\n",
    "        rf_loaded_data = pickle.load(f)\n",
    "\n",
    "    rf_loaded = rf_loaded_data.get(\"clf\") if isinstance(rf_loaded_data, dict) else rf_loaded_data\n",
    "    vector_size = rf_loaded_data.get(\"vector_size\", getattr(w2v_loaded, \"vector_size\", getattr(getattr(w2v_loaded, \"wv\", None), \"vector_size\", None)))\n",
    "\n",
    "    # choose a sample text (use one from your test set if available)\n",
    "    sample = \"This product exceeded my expectations and works perfectly.\"\n",
    "    tokens = simple_preprocess(sample, deacc=True)\n",
    "    # Access key_to_index for gensim >=4, fallback to vocabulary or similar for older versions\n",
    "    try:\n",
    "        key_index = getattr(w2v_loaded.wv, \"key_to_index\", None)\n",
    "        vecs = [w2v_loaded.wv[t] for t in tokens if key_index is None or t in key_index]\n",
    "    except Exception:\n",
    "        # If model was saved as older gensim object, try direct membership test\n",
    "        vecs = [w2v_loaded.wv[t] for t in tokens if t in w2v_loaded.wv]\n",
    "\n",
    "    if len(vecs) == 0:\n",
    "        warnings.warn(\"No in-vocabulary tokens found for sample text; verification skipped (OOV).\")\n",
    "    else:\n",
    "        avg_vec = np.mean(vecs, axis=0).reshape(1, -1)\n",
    "        pred = rf_loaded.predict(avg_vec)[0]\n",
    "        proba = rf_loaded.predict_proba(avg_vec)[0, 1] if hasattr(rf_loaded, \"predict_proba\") else None\n",
    "        print(\"Verification prediction:\", \"Positive\" if int(pred) == 1 else \"Negative\", f\"(prob={proba:.3f})\" if proba is not None else \"\")\n",
    "\n",
    "    print(\"Models saved successfully in project root. You can now run the Streamlit app which will load './w2v.pkl' and './rf_w2v.pkl'.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Verification failed after saving models: \" + str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
